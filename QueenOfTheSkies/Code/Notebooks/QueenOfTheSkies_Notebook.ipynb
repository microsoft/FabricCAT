{"cells":[{"cell_type":"markdown","id":"a93be367-5dd5-41a0-96d2-6db4178a2dc0","metadata":{},"source":["![logo](https://img-prod-cms-rt-microsoft-com.akamaized.net/cms/api/am/imageFileData/RE1Mu3b?ver=5c31)\n","\n","# **Fabric**\n","### Simulating streaming data for Realtime Analytics âš¡ using fabric Data Engineering notebook \n","### AKA: \"The Wood Chipper\" \n","This notebook will read any CSV you give it via the \"SampleCsv\" Parameter and will send it to an EventStream custom app endpoint (event hub). The notebook will send a certain number of lines for each batches according to the \"MyBatchSize\" Parameter. The number of batch size is computed automatically according to the total number of lines and batch size and the while loop will stop once the file has been streamed completely.  "]},{"cell_type":"markdown","id":"4dad526e-407d-4541-b7d2-d6af28447239","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["### **0. Set the parameters**"]},{"cell_type":"code","execution_count":null,"id":"0d894de5-a5a5-4dc8-b48d-5267651ab5a4","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# The connection string is what you get from the \"custom app\" endpoint in EventStream\n","MyConnectionString = 'My Custom App Endpoint'\n","\n","# This is the endpoint for where the CSV file is sitting.\n","SampleCsv = 'My ABFS file path'\n","\n","# Set batch size (i.e. number of rows from the CSV that being sent at once. use a higher number when wanting a more rapid movement on the report)\n","MyBatchSize = 12\n"]},{"cell_type":"markdown","id":"bcd9c142-f852-4c63-8070-3254b142edb1","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["### **1. Install dependencies and Event Hub library**"]},{"cell_type":"code","execution_count":null,"id":"5e09adb7-282c-474d-b31b-712a9d3273ee","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["pip install azure-eventhub>=5.11.0"]},{"cell_type":"code","execution_count":null,"id":"c1fe989f-60a1-4c78-83dc-f18235f0771e","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["import time\n","import os\n","import datetime\n","import json\n","import math\n","from azure.eventhub import EventHubProducerClient, EventData"]},{"cell_type":"markdown","id":"7e2da62d-0b89-4cd7-8652-d17c1bc422e0","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["### **2. Create a Python script to send events to your event stream**\n","\n","ref: https://learn.microsoft.com/azure/event-hubs/event-hubs-capture-python#create-a-python-script-to-send-events-to-your-event-hub"]},{"cell_type":"code","execution_count":null,"id":"61509439-23b7-4247-905e-384ca8d9ecdf","metadata":{"editable":true,"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"run_control":{"frozen":false}},"outputs":[],"source":["#Read in the CSV to a dataframe. \n","df = spark.read.csv(path=SampleCsv,header=True)\n","\n","#Instantiate an event hub producer\n","producer = EventHubProducerClient.from_connection_string(conn_str=MyConnectionString)\n","\n","# Determine the row count of the file\n","z = df.count() \n","\n","#Set some control variables\n","i            = MyBatchSize \n","x            = 0     # We open the batch at the first row by array index so we stat at 0\n","y            = x+i   # We seal the batch at Start + Increment(i)\n","BatchCounter = 0     # Initializing a batch counter\n","RowCounter   = 0     # Initializine a Row counter\n","TargetBatchCount = z/i if z%i ==0 else math.ceil(z/i) # Adding an additional batch if (RowCount / BatchSize) has a residual to catch them.\n","print ('====================================')\n","print ('Target batch count should be: '+ str(TargetBatchCount))\n","print ('====================================')\n","print ('Beginning stream...')\n","print ('====================================')\n","\n","while BatchCounter < TargetBatchCount:\n","\n","    BatchCounter = BatchCounter + 1 # == Mouve our batch counter one notch up  \n","    b = producer.create_batch()     # == Instantiate the batch\n","    j = df.toJSON().collect()[x:y]  # == Collect Rows from x to y and convert them to JSON\n","    for ii in range(0, len(j)):     # == We have to add every row in the batch individually to the event hub payload so Kusto can read it in.    \n","        b.add(EventData(j[ii]))     # == Add the JSON to the payload\n","    producer.send_batch(b)          # == Send the batch to Event hub!\n","    time.sleep(1)                   # == We add an intentional 1s pause\n","    producer.close()                # == Clean up the batch\n","    # Printing some stats to track the stream                \n","    print ('This was batch #:' + str(BatchCounter))\n","    print ('We loaded rows from: ' + str(x) + ' to row: ' + str(y)) \n","    #Setting the control variable for the next pass\n","    RowCounter   = RowCounter + i\n","    RowRemaining = max(0,(z-RowCounter))\n","    x = y\n","    y = x+i if RowRemaining > i else x+RowRemaining\n","    print ('Rows remaining in the stream: ' + str(RowRemaining))\n","    print ('====================================')\n","\n","print ('====================================')  \n","print ('End of stream reached')\n","print ('====================================')    \n","print ('Number of batches was: '  + str(BatchCounter))\n","print ('Last batch was from row: '+ str(x) + ' to row: '+ str(y)) \n"]}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","language":"Python","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"host":{},"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"notebook_environment":{},"nteract":{"version":"nteract-front-end@1.0.0"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{},"enableDebugMode":false}},"synapse_widget":{"state":{},"version":"0.1"},"trident":{"lakehouse":{"known_lakehouses":[]}},"widgets":{}},"nbformat":4,"nbformat_minor":5}
