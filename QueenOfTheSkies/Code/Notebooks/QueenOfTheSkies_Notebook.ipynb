{"cells":[{"cell_type":"markdown","source":["![logo](https://img-prod-cms-rt-microsoft-com.akamaized.net/cms/api/am/imageFileData/RE1Mu3b?ver=5c31)\n","\n","# **Fabric**\n","### Simulating streaming data for Realtime Analytics âš¡ using fabric Data Engineering notebook \n","### AKA: \"The Wood Chipper\" \n","This notebook will read any CSV you give it via the \"SampleCsv\" Parameter and will send it to an EventStream custom app endpoint (event hub). The notebook will send a certain number of lines for each batches according to the \"MyBatchSize\" Parameter. The number of batch size is computed automatically according to the total number of lines and batch size and the while loop will stop once the file has been streamed completely.  "],"metadata":{},"id":"a93be367-5dd5-41a0-96d2-6db4178a2dc0"},{"cell_type":"markdown","source":["### **0. Set the parameters**"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"4dad526e-407d-4541-b7d2-d6af28447239"},{"cell_type":"code","source":["# The connection string is what you get from the \"custom app\" endpoint in EventStream\n","MyConnectionString = ''\n","\n","# Set batch size (i.e. number of rows from the CSV that being sent at once. use a higher number when wanting a more rapid movement on the report)\n","MyBatchSize = 12\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"0d894de5-a5a5-4dc8-b48d-5267651ab5a4"},{"cell_type":"code","source":["import os\n","\n","# url to source csv file\n","FULL_URL = \"https://raw.githubusercontent.com/microsoft/FabricCAT/main/QueenOfTheSkies/Data/QueenOfTheSky_ex.csv\"\n","# lakehouse location -- assumes default lakehouse\n","LAKEHOUSE_FOLDER = \"/lakehouse/default\"\n","\n","# filename and data folders\n","CSV_FILE_NAME = \"QueenOfTheSky_ex.csv\"\n","DATA_FOLDER = \"Files/QoS\"\n","CSV_FILE_PATH = f\"{LAKEHOUSE_FOLDER}/{DATA_FOLDER}/\"\n","\n","if not os.path.exists(LAKEHOUSE_FOLDER):\n","    # add a lakehouse if the notebook has no default lakehouse\n","    # a new notebook will not link to any lakehouse by default\n","    raise FileNotFoundError(\n","        \"Lakehouse not found, please add a lakehouse for the notebook.\"\n","    )\n","else:\n","    # verify whether or not the required files are already in the lakehouse, and if not, download and unzip\n","    if not os.path.exists(f\"{CSV_FILE_PATH}{CSV_FILE_NAME}\"):\n","        print(f\"{CSV_FILE_PATH}{CSV_FILE_NAME}\")\n","        os.makedirs(CSV_FILE_PATH, exist_ok=True)\n","        os.system(f\"wget '{FULL_URL}' -O {CSV_FILE_PATH}{CSV_FILE_NAME}\")\n","\n","SampleCsv = f\"{DATA_FOLDER}/{CSV_FILE_NAME}\"\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"fee5173b-c6de-49a3-93e0-4c7e3df082c3"},{"cell_type":"markdown","source":["### **1. Install dependencies and Event Hub library**"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"bcd9c142-f852-4c63-8070-3254b142edb1"},{"cell_type":"code","source":["pip install azure-eventhub>=5.11.0"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"5e09adb7-282c-474d-b31b-712a9d3273ee"},{"cell_type":"code","source":["import time\n","import os\n","import datetime\n","import json\n","import math\n","from azure.eventhub import EventHubProducerClient, EventData"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"c1fe989f-60a1-4c78-83dc-f18235f0771e"},{"cell_type":"markdown","source":["### **2. Create a Python script to send events to your event stream**\n","\n","ref: https://learn.microsoft.com/azure/event-hubs/event-hubs-capture-python#create-a-python-script-to-send-events-to-your-event-hub"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"7e2da62d-0b89-4cd7-8652-d17c1bc422e0"},{"cell_type":"code","source":["#Read in the CSV to a dataframe. \n","df = spark.read.csv(path=SampleCsv,header=True)\n","\n","#Instantiate an event hub producer\n","producer = EventHubProducerClient.from_connection_string(conn_str=MyConnectionString)\n","\n","# Determine the row count of the file\n","z = df.count() \n","\n","#Set some control variables\n","i            = MyBatchSize \n","x            = 0     # We open the batch at the first row by array index so we stat at 0\n","y            = x+i   # We seal the batch at Start + Increment(i)\n","BatchCounter = 0     # Initializing a batch counter\n","RowCounter   = 0     # Initializine a Row counter\n","TargetBatchCount = z/i if z%i ==0 else math.ceil(z/i) # Adding an additional batch if (RowCount / BatchSize) has a residual to catch them.\n","print ('====================================')\n","print ('Target batch count should be: '+ str(TargetBatchCount))\n","print ('====================================')\n","print ('Beginning stream...')\n","print ('====================================')\n","\n","while BatchCounter < TargetBatchCount:\n","\n","    BatchCounter = BatchCounter + 1 # == Mouve our batch counter one notch up  \n","    b = producer.create_batch()     # == Instantiate the batch\n","    j = df.toJSON().collect()[x:y]  # == Collect Rows from x to y and convert them to JSON\n","    for ii in range(0, len(j)):     # == We have to add every row in the batch individually to the event hub payload so Kusto can read it in.    \n","        b.add(EventData(j[ii]))     # == Add the JSON to the payload\n","    producer.send_batch(b)          # == Send the batch to Event hub!\n","    time.sleep(1)                   # == We add an intentional 1s pause\n","    producer.close()                # == Clean up the batch\n","    # Printing some stats to track the stream                \n","    print ('This was batch #:' + str(BatchCounter))\n","    print ('We loaded rows from: ' + str(x) + ' to row: ' + str(y)) \n","    #Setting the control variable for the next pass\n","    RowCounter   = RowCounter + i\n","    RowRemaining = max(0,(z-RowCounter))\n","    x = y\n","    y = x+i if RowRemaining > i else x+RowRemaining\n","    print ('Rows remaining in the stream: ' + str(RowRemaining))\n","    print ('====================================')\n","\n","print ('====================================')  \n","print ('End of stream reached')\n","print ('====================================')    \n","print ('Number of batches was: '  + str(BatchCounter))\n","print ('Last batch was from row: '+ str(x) + ' to row: '+ str(y)) \n"],"outputs":[],"execution_count":null,"metadata":{"editable":true,"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"run_control":{"frozen":false}},"id":"61509439-23b7-4247-905e-384ca8d9ecdf"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"host":{},"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"widgets":{},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{},"enableDebugMode":false}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}},"trident":{"lakehouse":{"known_lakehouses":[{"id":"89f63502-3ed4-43db-9743-38e227a19c29"}],"default_lakehouse":"89f63502-3ed4-43db-9743-38e227a19c29","default_lakehouse_name":"QoS_Lakehouse","default_lakehouse_workspace_id":"e843cf12-8cba-4b82-8afa-2e25237bf599"}}},"nbformat":4,"nbformat_minor":5}